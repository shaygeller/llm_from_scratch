# PRD Appendix: Hardware-Constrained Implementation Specification

**Hardware:** 2× NVIDIA H100 80GB GPUs
**Time Budget:** 18-21 days total training time
**Compute Budget:** ~700-900 H100 GPU-hours

## 1. Training Throughput & Token Budget

### 1.1 Calculated Throughput
- **Model:** 1.1B parameters, BF16, FSDP FULL_SHARD
- **Batch Size:** 16 per GPU × 2 GPUs = 32 global batch size
- **Sequence Length:** 4K tokens (Phase 1 & 3), 8K-16K (Phase 2)
- **Estimated Speed:** 0.15-0.2 steps/second (~5-7 sec/step)
- **Daily Throughput:** ~12,960-17,280 steps/day

### 1.2 Token Budget Allocation

**Total Training Budget: ~30B tokens (230K steps)**

| Phase | Steps | Tokens | Duration | Seq Len | Batch Size |
|-------|-------|--------|----------|---------|------------|
| Phase 1 (0-80%) | 184K | 24B | 14-16 days | 4K | 32 |
| Phase 2 (80-90%) | 23K | 3B | 1-2 days | 8K→16K | 16→8 |
| Phase 3 (90-100%) | 23K | 3B | 1-2 days | 4K | 32 |
| **Total Pre-training** | **230K** | **30B** | **18-20 days** | - | - |
| SFT | 12K | ~200M | 2-3 days | 2K | 8 |
| DPO | 5K | ~40M | 1 day | 2K | 4 |
| **Grand Total** | **247K** | **~30.2B** | **21-24 days** | - | - |

**Critical Constraint:** 128K context training is infeasible on 2× H100. Maximum practical context during training is **16K tokens**.

## 2. Exact Dataset Specifications

### 2.1 Phase 1: General Knowledge (24B tokens)

**FineWeb-Edu (v1.0)**
- **Source:** `HuggingFaceFW/fineweb-edu`
- **Filter:** `score >= 3` (educational quality)
- **Language:** English only (`lang_score > 0.9`)
- **Deduplication:** MinHash, 9-gram shingles, Jaccard threshold 0.8
- **Quality Filters:**
  - Document length: 100 ≤ tokens ≤ 100K
  - Remove code-heavy docs (>30% code tokens)
  - PII filtering: regex for emails, phones, SSNs
  - Toxicity: Perspective API score < 0.5
- **Implementation:**
  ```python
  from datasets import load_dataset
  dataset = load_dataset('HuggingFaceFW/fineweb-edu',
                         split='train',
                         streaming=True)
  dataset = dataset.filter(lambda x: x['score'] >= 3)
  # Stream until 24B tokens consumed
  ```

**Validation Set:** Hold out 100M tokens for perplexity evaluation

### 2.2 Phase 2: Long Context Annealing (3B tokens)

**Sequence Length Ramping Schedule:**
- Steps 184K-194K (10K steps): 4K → 8K context (~1.3B tokens @ batch 16)
- Steps 194K-207K (13K steps): 8K → 16K context (~1.7B tokens @ batch 8)

**Datasets (50/50 mix):**

1. **PG19 Books - 1.5B tokens**
   - **Source:** `deepmind/pg19`
   - **Processing:** Full books (no truncation to preserve narrative flow)
   - **Why:** Long-form coherence, cross-chapter references

2. **The Stack Deduplicated - 1.5B tokens**
   - **Source:** `bigcode/the-stack-dedup`
   - **Languages:**
     - Python: 60% (~900M tokens)
     - JavaScript: 20% (~300M tokens)
     - TypeScript: 20% (~300M tokens)
   - **Filter:** Files with ≥ 1000 tokens (avoid snippets)
   - **Why:** Code dependencies span many lines, forces attention on long-range syntax

**Context Window Test:** After Phase 2, run needle-in-haystack evaluation:
- Hide random fact at random position in 16K document
- Target: ≥ 85% retrieval accuracy across 100 samples

### 2.3 Phase 3: Knowledge Crystallization (3B tokens)

**High-Quality "Gold" Sources:**

1. **OpenWebMath - 1.2B tokens**
   - **Source:** `open-web-math/open-web-math`
   - **Content:** Filtered math from web (arXiv, ProofWiki, math forums)
   - **Why:** Dense logical reasoning patterns

2. **StackExchange Curated - 800M tokens**
   - **Source:** `HuggingFaceH4/stack-exchange-preferences`
   - **Filter:**
     - Score ≥ 10
     - Accepted answers only
     - Topics: Mathematics, Computer Science, Physics, Statistics
   - **Why:** Expert-level technical Q&A

3. **STEM Wikipedia - 500M tokens**
   - **Source:** Wikipedia dumps (latest)
   - **Categories:** Mathematics, Physics, Computer Science, Engineering, Logic
   - **Processing:** Extract only technical articles (filter by category depth)
   - **Why:** High-quality technical writing

4. **Proof-Based Mathematics - 500M tokens**
   - **Source:** `EleutherAI/proof-pile-2` (subset)
   - **Content:** Formal mathematical proofs from arXiv
   - **Why:** Strongest logical reasoning signal

### 2.4 SFT Dataset (100K examples, ~200M tokens)

**Composition (50% / 25% / 25%):**

1. **General Chat - 50K examples**
   - **Source:** `OpenAssistant/oasst2`
   - **Filter:** English only, quality_score ≥ 0.7
   - **Alternative:** `HuggingFaceH4/ultrachat_200k` (sample 50K)

2. **Chain-of-Thought Math - 25K examples**
   - **Source:** `meta-math/MetaMathQA-400K` (sample 25K)
   - **Critical:** Exclude GSM8K test set (run decontamination)
   - **Format:** Question → `<|start_thought|>` step-by-step `<|end_thought|>` → Answer

3. **Tool Use - 25K examples**
   - **Option A (Existing):** `gorilla-llm/APIBench` (API calls)
   - **Option B (Synthetic):** Generate with GPT-4:
     ```
     System: Generate examples where user asks math question requiring calculator.
     Format:
     User: [question]
     Assistant: <|start_thought|>[reasoning]<|end_thought|>
                <|tool_call|>{"function": "calculate", "arguments": {"expression": "..."}}
                <|tool_response|>{"result": X}
                The answer is X.
     ```
   - **Tools Available:** Calculator, Python REPL, Web Search (mock for training)

**Loss Masking:** Compute loss only on tokens after `<|im_start|>assistant` until `<|im_end|>`

### 2.5 DPO Dataset (50K preference pairs)

**Source:** `Anthropic/hh-rlhf` (helpfulness + harmlessness)
- 50K pairs of (chosen, rejected) responses
- **Alternative:** `Intel/orca_dpo_pairs`

## 3. Complete Hyperparameter Specification

### 3.1 Model Architecture

```python
ModelConfig(
    vocab_size=32000,  # Llama-3 BPE tokenizer
    n_layers=22,
    hidden_dim=2048,
    n_heads=32,  # Query heads
    n_kv_heads=4,  # GQA ratio 8:1
    ffn_dim=5632,  # SwiGLU
    max_seq_len=16384,  # Base 4096, extended 16384
    rope_theta=10000.0,
    rope_theta_extended=500000.0,  # For 16K extension (not 10M)
    norm_eps=1e-5,  # RMSNorm epsilon
    attention_dropout=0.0,
    ffn_dropout=0.0,
    embedding_dropout=0.0,
    tie_word_embeddings=False,  # Separate input/output embeddings
)
```

**Special Tokens (extend vocab to 32008):**
```python
SPECIAL_TOKENS = {
    "pad_token": "<|pad|>",  # ID: 32000
    "bos_token": "<|begin_of_text|>",  # ID: 32001
    "eos_token": "<|end_of_text|>",  # ID: 1 (Llama standard)
    "im_start": "<|im_start|>",  # ID: 32002
    "im_end": "<|im_end|>",  # ID: 32003
    "start_thought": "<|start_thought|>",  # ID: 32004
    "end_thought": "<|end_thought|>",  # ID: 32005
    "tool_call": "<|tool_call|>",  # ID: 32006
    "tool_response": "<|tool_response|>",  # ID: 32007
}
```

### 3.2 Phase 1: Pre-training (230K steps)

```python
OptimizerConfig(
    name="AdamW",
    learning_rate=3e-4,
    betas=(0.9, 0.95),  # β1, β2
    eps=1e-8,
    weight_decay=0.1,
    grad_clip_norm=1.0,
)

SchedulerConfig(
    name="WSD",  # Warmup-Stable-Decay
    warmup_steps=2000,
    stable_steps=184000,  # Through Phase 1 (80%)
    decay_steps=44000,  # Phase 2 + 3 (20%)
    max_lr=3e-4,
    min_lr=3e-5,  # 10% of max
    decay_type="linear",
)

TrainingConfig(
    batch_size_per_gpu=16,
    n_gpus=2,
    global_batch_size=32,
    gradient_accumulation_steps=1,
    sequence_length=4096,  # Phase 1 & 3
    precision="bf16",

    # Stability
    z_loss_weight=1e-4,

    # FSDP
    sharding_strategy="FULL_SHARD",
    mixed_precision="bf16",
    activation_checkpointing=True,
    cpu_offload=False,

    # Checkpointing
    save_every_n_steps=5000,
    keep_last_n=3,
    async_save=True,

    # Logging
    log_every_n_steps=100,
    eval_every_n_steps=2500,

    # Seeds (for reproducibility)
    seed=42,
    cuda_deterministic=False,  # Slower if True
)
```

**Phase 2 Adjustments (Steps 184K-207K):**
```python
# Step 184K-194K: 8K context
batch_size_per_gpu = 8
sequence_length = 8192

# Step 194K-207K: 16K context
batch_size_per_gpu = 4
sequence_length = 16384
rope_theta = 500000.0  # Extended theta for 16K
```

### 3.3 Phase 2: SFT (12K steps)

```python
SFTConfig(
    learning_rate=1e-5,  # 10x lower than pre-training
    warmup_steps=100,
    total_steps=12000,  # ~3 epochs over 100K examples
    batch_size_per_gpu=4,
    global_batch_size=8,
    sequence_length=2048,  # Most chat < 2K
    grad_clip_norm=1.0,
    weight_decay=0.0,  # No weight decay for fine-tuning

    # Loss masking
    compute_loss_on_assistant_only=True,
    ignore_index=-100,  # Standard PyTorch convention

    # Regularization
    dropout=0.1,  # Add dropout during SFT to prevent overfitting
)
```

### 3.4 Phase 3: DPO (5K steps)

```python
DPOConfig(
    learning_rate=5e-7,  # Very low
    beta=0.1,  # KL divergence penalty
    warmup_steps=50,
    total_steps=5000,
    batch_size_per_gpu=2,  # Needs 2x memory (chosen + rejected)
    global_batch_size=4,
    sequence_length=2048,
    grad_clip_norm=1.0,

    # Memory optimization
    reference_model_device="cpu",  # Offload reference model
    label_smoothing=0.0,

    # Loss
    loss_type="sigmoid",  # Standard DPO loss
)
```

## 4. Success Criteria & Benchmarks

### 4.1 Pre-training (Phase 1) Validation

**Metric:** Perplexity on held-out FineWeb-Edu validation set (100M tokens)

| Checkpoint | Target PPL | Status |
|------------|-----------|--------|
| 50K steps | < 25 | Model is learning |
| 100K steps | < 18 | Convergence starting |
| 150K steps | < 15 | On track |
| 200K steps | < 13 | Good |
| **230K steps (final)** | **< 12** | **Success** |

**Red Flags (abort if seen):**
- Loss NaN or Inf after step 5000
- Gradient norm > 100 (explosion)
- PPL not decreasing after 20K steps
- Throughput < 0.1 steps/sec (efficiency issue)

### 4.2 Long-Context Validation (Phase 2 end, Step 207K)

**Needle-in-Haystack Test @ 16K Context:**
- Hide random fact at random position in 16K token document
- Query: "What is [hidden fact]?"
- **Success:** ≥ 85% accuracy across 100 test samples

**If failed:** Attention mechanism bug (check RoPE implementation, causal mask)

### 4.3 SFT Benchmarks (Before DPO)

**Target vs Baselines:**

| Benchmark | Random | TinyLlama-1.1B (3T) | MobileLLM-1B (1T) | **Our Target (30B)** |
|-----------|--------|---------------------|-------------------|----------------------|
| **MMLU (5-shot)** | 25% | 25.3% | 38.0% | **30-35%** (Stretch: 38-40%) |
| **GSM8K (8-shot CoT)** | ~0% | 6.0% | ~12% | **15-20%** (Stretch: 25%) |
| **HellaSwag (0-shot)** | 25% | 59.2% | - | **50-55%** |
| **ARC-Challenge** | 25% | 30.7% | - | **30-35%** |

**Tool Use Evaluation (Custom):**
- 200 held-out tool use examples
- **Syntax Validity:** ≥ 95% (with grammar-constrained decoding)
- **Semantic Correctness:** ≥ 60-70% (right function + right arguments)

**Qualitative Checks:**
- Inspect 50 random `<|start_thought|>` generations for coherent reasoning
- No degeneration (repetition loops, gibberish)
- Appropriate refusals (refuses harmful requests, not math questions)

### 4.4 DPO Benchmarks

**Alignment Evaluation:**

| Benchmark | SFT Baseline | **DPO Target** |
|-----------|--------------|----------------|
| **AlpacaEval (win rate)** | 50% (vs itself) | **≥ 60%** (DPO vs SFT) |
| **MT-Bench (0-10 scale)** | ~4.0 | **≥ 4.5** |
| **TruthfulQA** | ~30% | **≥ 35%** |
| **RealToxicityPrompts** | Baseline | **-20% reduction** |

### 4.5 Final Success Declaration

**Minimum Viable Product (MVP):**
- ✅ Pre-training PPL < 12
- ✅ MMLU ≥ 30% (better than random + TinyLlama)
- ✅ GSM8K ≥ 15% (3× better than TinyLlama)
- ✅ Tool use semantic accuracy ≥ 60%
- ✅ DPO improves over SFT on AlpacaEval

**Success (Validates "Quality > Quantity"):**
- ✅ **MMLU ≥ 35%** with only 30B tokens (vs MobileLLM 38% with 1T tokens = 33× less data)
- ✅ **GSM8K ≥ 20%** (proves reasoning curriculum works)
- ✅ Needle-in-haystack ≥ 85% @ 16K context
- ✅ Interpretable reasoning in thought blocks

**Stretch Goals (Publication-Worthy):**
- ✅ MMLU ≥ 40% (matching 1.6B models with 1B params)
- ✅ GSM8K ≥ 25%
- ✅ Paper title: "1B Parameters, 30B Tokens: How Curriculum Design Outperforms Scale"

## 5. Decontamination & Reproducibility

### 5.1 N-gram Decontamination (CRITICAL)

**Before training, check 13-gram overlaps:**

```python
from datasketch import MinHash

def decontaminate(train_dataset, test_datasets, n=13, threshold=0.8):
    """
    Remove training examples with >80% 13-gram overlap with test sets
    """
    test_hashes = []
    for test_set in test_datasets:  # MMLU, GSM8K, HellaSwag, ARC
        for example in test_set:
            m = MinHash(num_perm=128)
            tokens = tokenize(example['text'])
            for i in range(len(tokens) - n + 1):
                ngram = tuple(tokens[i:i+n])
                m.update(str(ngram).encode())
            test_hashes.append(m)

    contaminated_indices = set()
    for idx, train_example in enumerate(train_dataset):
        m = MinHash(num_perm=128)
        tokens = tokenize(train_example['text'])
        for i in range(len(tokens) - n + 1):
            ngram = tuple(tokens[i:i+n])
            m.update(str(ngram).encode())

        for test_m in test_hashes:
            if m.jaccard(test_m) > threshold:
                contaminated_indices.add(idx)
                break

    clean_dataset = train_dataset.filter(
        lambda x, idx: idx not in contaminated_indices,
        with_indices=True
    )

    print(f"Removed {len(contaminated_indices)} contaminated examples")
    return clean_dataset
```

**Run before Phase 1 training:**
- Check FineWeb-Edu against MMLU, GSM8K, HellaSwag, ARC test sets
- Check MetaMathQA against GSM8K test set (for SFT)

### 5.2 Reproducibility Specification

```python
# Exact environment
python_version = "3.11.5"
pytorch_version = "2.4.0" or "2.5.0-nightly"
cuda_version = "12.4"
transformers_version = "4.44.0"
flash_attn_version = "2.6.0"

# Seeds
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
torch.cuda.manual_seed_all(42)

# For full reproducibility (slower):
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# Save exact package versions:
# pip freeze > requirements_exact.txt
```

**Checkpoint Contents:**
- `model.safetensors` (model weights)
- `optimizer.pt` (AdamW state)
- `scheduler.pt` (WSD state)
- `rng_state.pt` (torch, numpy, python random states)
- `training_state.json` (step, tokens_seen, phase, sequence_length)
- `dataloader_state.json` (dataset position for resuming)

## 6. Monitoring & Debugging

### 6.1 Logging (Weights & Biases)

```python
wandb.init(
    project="nanollama-1b",
    name=f"run_{timestamp}",
    config={
        "model_params": 1_100_000_000,
        "vocab_size": 32000,
        "total_tokens": "30B",
        "hardware": "2x_H100_80GB",
        "phases": ["pretrain", "sft", "dpo"],
    }
)

# Log every 100 steps:
wandb.log({
    "loss": loss.item(),
    "perplexity": torch.exp(loss).item(),
    "learning_rate": scheduler.get_last_lr()[0],
    "gradient_norm": grad_norm,
    "tokens_per_second": tokens_per_sec,
    "mfu_percent": model_flops_utilization,
    "gpu_memory_gb": torch.cuda.max_memory_allocated() / 1e9,
    "z_loss": z_loss.item(),
    "step": global_step,
    "tokens_seen": global_step * batch_size * seq_len,
})

# Log histograms every 1000 steps:
wandb.log({
    "gradients": wandb.Histogram(all_gradients),
    "attention_scores": wandb.Histogram(attn_weights),  # Check if using full context
})
```

### 6.2 Alert System

**Conditions to Alert/Abort:**
1. Loss NaN or Inf (critical)
2. Gradient norm > 100 for 3 consecutive steps (explosion)
3. Learning rate = 0 before final step (scheduler bug)
4. GPU memory OOM (need to reduce batch size)
5. Throughput < 0.08 steps/sec for 10 minutes (efficiency issue)
6. Perplexity increasing for 5K consecutive steps (divergence)

### 6.3 Debugging Tools

**Attention Pattern Visualization:**
```python
# After Phase 2, verify model uses full 16K context
def visualize_attention(model, input_ids):
    with torch.no_grad():
        outputs = model(input_ids, output_attentions=True)
        # Plot attention weights for layer 11 (middle)
        attn = outputs.attentions[11][0, 0]  # [seq, seq]
        plt.imshow(attn.cpu(), cmap='viridis')
        plt.title('Attention Pattern (16K context)')
        plt.show()

    # Expected: Diagonal band (causal) + some long-range connections
    # Bug if: Only attending to first 4K tokens
```

## 7. Cost & Time Estimates

**Total Training Time:**
- Pre-training: 18-20 days
- SFT: 2-3 days
- DPO: 1 day
- **Total: 21-24 days**

**GPU-Hours:**
- 2× H100 × 24 days × 24 hours = **1,152 H100-hours**

**Cost (if on cloud):**
- H100 cloud cost: ~$2.50/hour
- Total: 1,152 × $2.50 = **~$2,880**
- (User has own GPUs, so actual cost = $0)

**Energy Cost (estimate):**
- H100 TDP: 700W per GPU
- 2× H100 × 700W × 24 days × 24 hours = 806 kWh
- At $0.12/kWh: **~$97 electricity cost**

## 8. Known Risks & Mitigation

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| Training diverges (NaN loss) | Medium | Critical | z-loss, gradient clipping, checkpoint rollback |
| OOM during 16K context | Medium | High | Reduce batch size to 4, use Flash Attention 2 |
| Poor benchmark scores | Medium | High | Curriculum ablation, extend Phase 3 by 5K steps |
| Data contamination | Low | Critical | Run 13-gram decontamination before training |
| Hardware failure | Low | High | Checkpoint every 5K steps, async save |
| Dataset download failure | Medium | Medium | Pre-download all datasets, cache locally |
| Overfitting during SFT | Medium | Medium | Add dropout (0.1), early stopping on validation loss |

---

**This appendix provides complete, executable specifications for implementing the NanoLlama-1B project on 2× H100 GPUs within a 3-week timeline.**
